{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я возьму для рассмотрения stanza, pymorphy и natasha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 12:01:13 INFO: Downloading default packages for language: ru (Russian)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 12:01:16 INFO: File exists: C:\\Users\\User\\stanza_resources\\ru\\default.zip.\n",
      "2021-10-03 12:01:22 INFO: Finished downloading models and saved to C:\\Users\\User\\stanza_resources.\n",
      "2021-10-03 12:01:22 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2021-10-03 12:01:22 INFO: Use device: cpu\n",
      "2021-10-03 12:01:22 INFO: Loading: tokenize\n",
      "2021-10-03 12:01:27 INFO: Loading: pos\n",
      "2021-10-03 12:01:27 INFO: Loading: lemma\n",
      "2021-10-03 12:01:28 INFO: Loading: depparse\n",
      "2021-10-03 12:01:28 INFO: Loading: ner\n",
      "2021-10-03 12:01:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "import stanza\n",
    "from sklearn.metrics import accuracy_score\n",
    "from string import punctuation\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation += \"–«»…\"\n",
    "punctuation = punctuation.replace(\"_\", \"\")\n",
    "punctuation = punctuation.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпуса я собрала несколько предложений из Лимонова, потому что у него специфичный стиль, много окказионализмов и иногда встречается что-то интересное по синтаксису."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Комнатка_NOUN моя_PRON_ADJ имеет_VERB 4_NUM шага_NOUN в_PREP длину_NOUN и_CONJ 3_NUM в_PREP ширину_NOUN. На_PREP стенах_NOUN, прикрывая_ADV_VERB пятна_NOUN, оставшиеся_ADJ_VERB от_PREP прежних_ADJ жильцов_NOUN, висят_VERB: большой_ADJ портрет_NOUN Мао_NOUN Цзэ_NOUN Дуна_NOUN; портрет_NOUN Патриции_NOUN'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('eto_ya_edichka.txt', encoding='UTF-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text[:303] #сам корпус размечен примерно так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Комнатка моя имеет 4 шага в длину и 3 в ширину. На стенах, прикрывая пятна, оставшиеся от прежних жильцов, висят: большой портрет Мао Цзэ Дуна; портрет Патриции Хёрст; портрет Андре Бретона, основателя сюрреалистической школы, который я вожу с собой уже много лет, и которого Андре Бретона обычно никто их приходящих ко мне не знает; плакат, призывающий голосовать за Рабочей партии кандидатов; картины моего друга художника Хачатуряна. В изголовье кровати у меня плакат – «За Вашу и Нашу свободу», оставшийся от демонстрации у здания «Нью Йорк Таймз». Ну я был поэт, поэт я был, раз уж вам хочется знать кто, неофициальный поэт был, подпольный, было да сплыло, а теперь я один из ваших, я подонок, я тот кого вы кормите щами, кого вы поите дешёвым и дрянным калифорнийским вином – 3.59 галлоновая бутыль, а я все равно вас презираю.Остальной персонал отеля относится ко мне так-сяк.Я не знаю, какие там комнаты, но место там похуже, куда более блатное.Он перебивается с хлеба на воду, худ как скелет, но работать почему-то не идёт.У меня не высыхал пот на лбу, я не даром получал мои чаевые. Куда как не даром. Впрочем, беготне этой я был рад первое время.Она объявила мне это вечером 19 декабря в нашей лексингтоновской трагической квартире.Я глазею на знаменитого на весь отель соученика Фиделя Кастро – долговязого кубинца-швейцара в красном форменном пальто, открывающего дверцы подходящих к отелю автомобилей.Упомянутый в солженицыновском «Архипелаге Гулаг» Лёня Косогор, высокий, сутулый человек пятидесяти с лишним лет работает электриком – они ходят в светло-зелёных мешковатых робах.Ну, как тебе нравится Раймончик, Эдичка? Не правда ли, он душка?'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_show = re.sub(\"_[A-Z]{3,4}|\\n\", \"\", text)\n",
    "text_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут много имён собственных, разговорных слов («так-сяк»), слов, которые могут выступать как несколько частей речи («как» союз и частица, «правда» существительное и частица), есть редкая форма «худ», всякие явно несловарные слова («галлоновый», «лексингтоновский»), и слова, написанные через дефис («кубинец-швейцар» и «светло-зелёный»)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_wo_punct = text.translate(str.maketrans('', '', punctuation))\n",
    "text_wo_punct = re.sub('\\n', ' ', text_wo_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Комнатка', ['NOUN']],\n",
       " ['моя', ['PRON', 'ADJ']],\n",
       " ['имеет', ['VERB']],\n",
       " ['4', ['NUM']],\n",
       " ['шага', ['NOUN']],\n",
       " ['в', ['PREP']],\n",
       " ['длину', ['NOUN']],\n",
       " ['и', ['CONJ']],\n",
       " ['3', ['NUM']],\n",
       " ['в', ['PREP']]]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_POS = []\n",
    "text_rem = text_wo_punct\n",
    "while text_rem:\n",
    "    tag_pos = re.search(\"(_[A-Z]{3,4}){1,2}\", text_rem)\n",
    "    word = text_rem[:tag_pos.start(0)]\n",
    "    gold_POS.append([word, tag_pos.group(0)[1:]])\n",
    "    text_rem = text_rem[tag_pos.end(0):].lstrip()\n",
    "    \n",
    "for i in range(len(gold_POS)):\n",
    "    gold_POS[i][1] = gold_POS[i][1].split('_')\n",
    "    \n",
    "gold_POS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_wo_punct = re.sub(\"_[A-Z]{3,4}\", \"\", text_wo_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stanza_doc = nlp(text_wo_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Комнатка', 'NOUN'],\n",
       " ['моя', 'DET'],\n",
       " ['имеет', 'VERB'],\n",
       " ['4', 'NUM'],\n",
       " ['шага', 'NOUN'],\n",
       " ['в', 'ADP'],\n",
       " ['длину', 'NOUN'],\n",
       " ['и', 'CCONJ'],\n",
       " ['3', 'NUM'],\n",
       " ['в', 'ADP']]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stanza_POS = []\n",
    "for sent in stanza_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        stanza_POS.append([word.text, word.upos])\n",
    "        \n",
    "stanza_POS = [i for i in stanza_POS if i[0] not in punctuation+'-']\n",
    "\n",
    "stanza_POS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Комнатка', 'PROPN'],\n",
       " ['моя', 'DET'],\n",
       " ['имеет', 'VERB'],\n",
       " ['4', 'NUM'],\n",
       " ['шага', 'NOUN'],\n",
       " ['в', 'ADP'],\n",
       " ['длину', 'NOUN'],\n",
       " ['и', 'CCONJ'],\n",
       " ['3', 'NUM'],\n",
       " ['в', 'ADP']]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natasha_doc = Doc(text_wo_punct)\n",
    "\n",
    "natasha_doc.segment(segmenter)\n",
    "natasha_doc.tag_morph(morph_tagger)\n",
    "natasha_POS = []\n",
    "for token in natasha_doc.tokens:\n",
    "    natasha_POS.append([token.text, token.pos])\n",
    "natasha_POS = [i for i in natasha_POS if i[1] != 'PUNCT']\n",
    "\n",
    "natasha_POS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Комнатка', 'NOUN'],\n",
       " ['моя', 'ADJF'],\n",
       " ['имеет', 'VERB'],\n",
       " ['4', None],\n",
       " ['шага', 'NOUN'],\n",
       " ['в', 'PREP'],\n",
       " ['длину', 'NOUN'],\n",
       " ['и', 'CONJ'],\n",
       " ['3', None],\n",
       " ['в', 'PREP']]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymor_POS = []\n",
    "for word in text_wo_punct.split():\n",
    "    pymor_POS.append([word, morph.parse(word.lower())[0].tag.POS])\n",
    "    \n",
    "pymor_POS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала приведём теги к одному виду. Я размечала наиболее привычными для себя тегами, в основном они совпадают с pymorphy2, только у меня в стандарте нет разделения прилагательных и причастий на краткие/полные, разделения союзов и прочего. Местоимения имеют как свой собственный тег, так и тег той части речи, на которую они похожи (например, ADJ для «который»), потому что какой-то из теггеров так их размечал, а также двойные теги у деепричастий (наречие+глагол) и причастий (прилагательное+глагол), потому что, опять же, теггеры рассматривали их по-разному и не хотелось какую-то точку зрения обижать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_levelling(tagset):\n",
    "    tags = [i[1] for i in tagset]\n",
    "    for i in range(len(tags)):\n",
    "        if str(tags[i]) == \"PROPN\":\n",
    "            tags[i] = \"NOUN\"\n",
    "        elif str(tags[i]) == \"ADP\":\n",
    "            tags[i] = \"PREP\"\n",
    "        elif str(tags[i]) in [\"DET\", \"NPRO\"]:\n",
    "            tags[i] = \"PRON\"\n",
    "        elif str(tags[i]) in [\"CCONJ\", \"SCONJ\"]:\n",
    "            tags[i] = \"CONJ\"\n",
    "        elif str(tags[i]) in [\"AUX\", \"INFN\", \"GRND\"]:\n",
    "            tags[i] = \"VERB\"\n",
    "        elif str(tags[i]) in [\"ADJF\", \"ADJS\", \"PRTF\", \"PRTS\"]:\n",
    "            tags[i] = \"ADJ\"\n",
    "        elif str(tags[i]) == \"PRCL\":\n",
    "            tags[i] = \"PART\"\n",
    "        elif str(tags[i]) == \"ADVB\":\n",
    "            tags[i] = \"ADV\"\n",
    "        elif str(tags[i]) == \"NUMR\":\n",
    "            tags[i] = \"NUM\"\n",
    "        else:\n",
    "            tags[i] = str(tags[i])\n",
    "        \n",
    "    for i in range(len(tagset)):\n",
    "        tagset[i][1] = tags[i]\n",
    "    \n",
    "    return tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_POS = tag_levelling(stanza_POS)\n",
    "natasha_POS = tag_levelling(natasha_POS)\n",
    "pymor_POS = tag_levelling(pymor_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас все теги одного вида, можно их сравнивать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_comparison(a):\n",
    "    \n",
    "    m = deepcopy(gold_POS)\n",
    "    \n",
    "    # m — manually tagged\n",
    "    # a — automatically tagged\n",
    "    \n",
    "    auto_word = [i[0] for i in a]\n",
    "    auto_tag = [i[1] for i in a]\n",
    "    tags_to_compare = []\n",
    "    for i in range(len(m)):\n",
    "        word = m[i][0]\n",
    "        if word == auto_word[0]:\n",
    "            tags_to_compare.append(auto_tag[0])\n",
    "            auto_word = auto_word[1:]\n",
    "            auto_tag = auto_tag[1:]\n",
    "        # на случай, если слово с дефисом разделено на два\n",
    "        elif word == auto_word[0]+'-'+auto_word[1]:\n",
    "            tags_to_compare.append(auto_tag[1])\n",
    "            auto_word = auto_word[2:]\n",
    "            auto_tag = auto_tag[2:]\n",
    "            \n",
    "        # если есть два варианта разбора слова, оставим тот, который совпадает с парсером, либо просто последний. \n",
    "        if len(m[i][1]) > 1:\n",
    "            if m[i][1][0] == tags_to_compare[-1]:\n",
    "                m[i][1] = [m[i][1][0]]\n",
    "            else:\n",
    "                m[i][1] = [m[i][1][1]]\n",
    "                \n",
    "        gold_tags = [i[1][0] for i in m]\n",
    "        \n",
    "            \n",
    "    return tags_to_compare, gold_tags     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANZA\t\t 0.9427480916030534\n",
      "NATASHA\t\t 0.9007633587786259\n",
      "PYMORPHY2\t 0.9236641221374046\n"
     ]
    }
   ],
   "source": [
    "stanza_tags, gold_stanza_tags = for_comparison(stanza_POS)\n",
    "natasha_tags, gold_natasha_tags = for_comparison(natasha_POS)\n",
    "pymor_tags, gold_pymor_tags = for_comparison(pymor_POS)\n",
    "\n",
    "print('STANZA\\t\\t', accuracy_score(gold_stanza_tags, stanza_tags))\n",
    "print('NATASHA\\t\\t', accuracy_score(gold_natasha_tags, natasha_tags))\n",
    "print('PYMORPHY2\\t', accuracy_score(gold_pymor_tags, pymor_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший теггер — stanza. Соберём n-граммы:\n",
    "1. прилагательное + существительное + существительное\n",
    "2. наречие + прилагательное\n",
    "3. \"я\" + глагол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_noun_noun = Counter()\n",
    "adv_adj = Counter()\n",
    "ya_verb = Counter()\n",
    "for i in range(len(stanza_POS)-1):\n",
    "    word, tag = stanza_POS[i][0], stanza_POS[i][1]\n",
    "    next_word, next_tag = stanza_POS[i+1][0], stanza_POS[i+1][1]\n",
    "    if i <= len(stanza_POS)-3:\n",
    "        next_next_word, next_next_tag = stanza_POS[i+2][0], stanza_POS[i+2][1]\n",
    "        if tag == \"ADJ\" and next_tag == 'NOUN' and next_next_tag == \"NOUN\":\n",
    "            adj_noun_noun[word+\" \"+next_word+\" \"+next_next_word] += 1\n",
    "    if tag == \"ADV\" and next_tag == \"ADJ\":\n",
    "        adv_adj[word+\" \"+next_word] += 1\n",
    "    elif word.lower() == \"я\" and next_tag == \"VERB\":\n",
    "        ya_verb[\"я \"+next_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'большой портрет Мао': 1,\n",
       "          'Рабочей партии кандидатов': 1,\n",
       "          'Остальной персонал отеля': 1,\n",
       "          'долговязого кубинца швейцара': 1,\n",
       "          'солженицыновском Архипелаге Гулаг': 1}),\n",
       " Counter({'там похуже': 1, 'более блатное': 1}),\n",
       " Counter({'я вожу': 1, 'я был': 3, 'я глазею': 1}))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_noun_noun, adv_adj, ya_verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение предыдущей домашки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К тем n-граммам, которые могли бы помочь в прошлый раз при определении тональности в отзывах на фильмы, можно отнести **не+глагол** (в отрицательных часто можно встретить «не зацепило», «не удалось», тем более «не понравилось»), **наречие+прилагательное** (скажем, условное «ужасно интересно», в котором при разделении точно обманчив окрас одного из элементов), и что-то типа **[фильм]+глагол+глагол/существительное/прилагательное(предикат)** — таких формулировок много при подведении итога («Фильм заставляет задуматься», «Картина вызывает отвращение», «„Оно“ получилось спорным» — правда, последняя n-грамма будет полезна только при сильно большом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza неприлично долго обрабатывает тексты, поэтому собственно улучшения предыдущей программы не получилось :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
